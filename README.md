# ğŸš€ Groq AI Advanced

![Python](https://img.shields.io/badge/Python-3.11-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-Production-green)
![Docker](https://img.shields.io/badge/Docker-Ready-blue)
![License](https://img.shields.io/badge/License-MIT-yellow)
![Groq](https://img.shields.io/badge/Powered%20by-Groq-black)

Production-ready local backend that integrates **Groq LLM API** with:

- âœ… Persistent conversation memory (SQLite)
- âœ… Basic RAG (Retrieval Augmented Generation)
- âœ… Rate limiting
- âœ… Internal API authentication
- âœ… Structured logging
- âœ… CLI interface
- âœ… Docker support
- âœ… Deployment-ready architecture

---

# ğŸ“Œ Important

> This application runs locally, but model inference runs on Groq's infrastructure.

Models are **not downloaded locally**.  
All inference happens via:

```
https://api.groq.com/
```

---

# ğŸ§  What is Groq?

Groq provides ultra-fast LLM inference using specialized **LPU hardware**, optimized for:

- âš¡ Low latency
- ğŸš€ High throughput
- ğŸ’° Cost efficiency
- ğŸ§  Large-scale model inference

Access models via API key:

ğŸ‘‰ https://console.groq.com/

---

# ğŸ”‘ How to Create a Groq API Key

1. Go to: https://console.groq.com/
2. Sign up or log in
3. Navigate to **API Keys**
4. Click **Create API Key**
5. Copy your key
6. Add it to your `.env` file

âš ï¸ Never commit your API key to GitHub.

---

# âš™ï¸ Environment Configuration

Create a file named `.env`:

```env
GROQ_API_KEY=your_real_key_here
GROQ_MODEL=groq/compound-mini
INTERNAL_API_KEY=local_secure_key
```

---

## ğŸ” Environment Variables Explained

| Variable | Description |
|-----------|------------|
| `GROQ_API_KEY` | Your secret Groq API key |
| `GROQ_MODEL` | Model name to use |
| `INTERNAL_API_KEY` | Authentication key for local API access |

---

# ğŸ¤– Available Models

| Model | Requests/Day | Tokens/Min | Use Case | Cost Level |
|-------|--------------|------------|----------|------------|
| `groq/compound-mini` | 250 | 70k | Testing, CLI, lightweight apps | ğŸŸ¢ Lowest |
| `groq/compound` | 250 | 70k | More structured responses | ğŸŸ¡ Medium |
| `llama3-8b` | 14,400 | 6k | General purpose | ğŸŸ¡ Medium |
| `llama-3.3-70b` | 1,000 | 12k | Advanced reasoning | ğŸ”´ Higher |
| `llama-guard` | â€” | â€” | Moderation only | â€” |

---

## ğŸ¯ Default Model Used

```
groq/compound-mini
```

Why?

- Lowest cost
- High token throughput
- Ideal for portfolio projects
- Good balance performance

---

# ğŸ— Architecture Overview

```
Client (CLI / HTTP)
        â”‚
        â–¼
FastAPI Backend
        â”‚
        â–¼
Rate Limiter
        â”‚
        â–¼
Authentication
        â”‚
        â–¼
SQLite Memory
        â”‚
        â–¼
RAG (TF-IDF Retrieval)
        â”‚
        â–¼
Groq API
        â”‚
        â–¼
LLM Inference (Cloud)
        â”‚
        â–¼
Response
```

---

# ğŸ“‚ Project Structure

```
groq-ai-advanced/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ groq_client.py
â”‚   â”œâ”€â”€ routes.py
â”‚   â”œâ”€â”€ memory.py
â”‚   â”œâ”€â”€ rag.py
â”‚   â”œâ”€â”€ rate_limiter.py
â”‚   â”œâ”€â”€ auth.py
â”‚   â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ knowledge.txt
â”‚
â”œâ”€â”€ cli.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

# ğŸ§  Core Features

---

## 1ï¸âƒ£ Persistent Memory (SQLite)

- Stores user + AI messages
- Retrieves last conversation context
- Enables contextual dialogue

File:
```
memory.db
```

---

## 2ï¸âƒ£ RAG (Retrieval Augmented Generation)

- Loads knowledge base from:
```
data/knowledge.txt
```
- Uses TF-IDF vectorization
- Retrieves most relevant context
- Injects into LLM prompt

Improves domain-specific accuracy.

---

## 3ï¸âƒ£ Rate Limiting

- 200 requests per 24 hours
- Prevents API overuse
- Protects cost exposure

---

## 4ï¸âƒ£ Internal API Authentication

All `/chat` requests must include header:

```
x-api-key: your_internal_key
```

Prevents unauthorized usage.

---

## 5ï¸âƒ£ Logging

Structured logging via Python logging module.

Extendable with:

- JSON logs
- Monitoring systems
- Error tracking tools

---

# ğŸš€ Installation

## 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/yourusername/groq-ai-advanced.git
cd groq-ai-advanced
```

---

## 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
```

Activate:

Mac/Linux:
```bash
source venv/bin/activate
```

Windows:
```bash
venv\Scripts\activate
```

---

## 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## 4ï¸âƒ£ Configure Environment

Copy:

```bash
cp .env.example .env
```

Edit `.env` and insert your real keys.

---

# â–¶ Run Local API

```bash
uvicorn app.main:app --reload
```

Access Swagger docs:

```
http://127.0.0.1:8000/docs
```

---

# â–¶ Run CLI Mode

```bash
python cli.py
```

---

# ğŸ³ Run with Docker

## Build image

```bash
docker build -t groq-ai .
```

## Run container

```bash
docker run -p 8000:8000 groq-ai
```

---

# ğŸ’° Cost Control Strategy

To reduce cost:

- Limit `max_tokens`
- Keep prompts concise
- Use RAG for precision
- Avoid unnecessary retries
- Monitor daily usage

---

# ğŸ”’ Security Best Practices

- Never expose `GROQ_API_KEY`
- Keep `.env` in `.gitignore`
- Rotate keys periodically
- Use backend-only API calls
- Protect deployed endpoint

---

# ğŸ›  Production Improvements (Future)

- Redis-based distributed rate limiting
- PostgreSQL instead of SQLite
- Real embeddings (not TF-IDF)
- Streaming responses (SSE)
- React frontend
- Multi-user authentication
- SaaS billing integration
- CI/CD pipeline
- Monitoring (Prometheus / Grafana)

---

# ğŸ“œ License

MIT License

---

# ğŸ‘¤ Author

Your Name  
Built with Groq + FastAPI  

---

# â­ If you like this project

Give it a star â­ on GitHub.